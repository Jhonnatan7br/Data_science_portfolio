# Hard Luxury Paris housing 

The dataset contains information about housing properties in Paris. Here are the columns and their descriptions:

- squareMeters: The area of the house in square meters.
- numberOfRooms: The number of rooms in the house.
- hasYard: A binary variable indicating whether the house has a yard (1 if yes, 0 otherwise).
- hasPool: A binary variable indicating whether the house has a pool (1 if yes, 0 otherwise).
- floors: The number of floors in the house.
- cityCode: A code representing the city where the house is located.
- cityPartRange: A range value representing a part of the city.
- numPrevOwners: The number of previous owners.
- made: The year the house was built.
- isNewBuilt: A binary variable indicating whether the house is newly built (1 if yes, 0 otherwise).
- hasStormProtector: A binary variable indicating whether the house has a storm protector (1 if yes, 0 otherwise).
- basement: The area of the basement in square meters.
- attic: The area of the attic in square meters.
- garage: The area of the garage in square meters.
- hasStorageRoom: A binary variable indicating whether the house has a storage room (1 if yes, 0 otherwise).
- hasGuestRoom: The number of guest rooms in the house.
- price: The price of the house.
- category: The luxury category of the house (Basic or Luxury).


   squareMeters  numberOfRooms  hasYard  hasPool  floors  cityCode  \
0         75523              3        0        1      63      9373   
1         80771             39        1        1      98     39381   
2         55712             58        0        1      19     34457   
3         32316             47        0        0       6     27939   
4         70429             19        1        1      90     38045   

   cityPartRange  numPrevOwners  made  isNewBuilt  hasStormProtector  \
0              3              8  2005           0                  1   
1              8              6  2015           1                  0   
2              6              8  2021           0                  0   
3             10              4  2012           0                  1   
4              3              7  1990           1                  0   

   basement  attic  garage  hasStorageRoom  hasGuestRoom      price category  
0      4313   9005     956               0             7  7559081.5    Basic  
1      3653   2436     128               1             2  8085989.5   Luxury  
2      2937   8852     135               1             9  5574642.1    Basic  
3       659   7141     359               0             3  3232561.2   Luxury  
4      8435   2429     292               1             4  7055052.0   Luxury 

# Missing values
The next step in the data-cleaning process is to check for missing values. In this case, the dataset does not contain any missing values in any of the columns. This is great because it means we don't have to deal with imputation or row deletion strategies for this dataset.

<img width="321" alt="image" src="https://github.com/Jhonnatan7br/Hard-Luxury-Paris-Housing/assets/104907786/a02a224e-979e-4e22-90b5-95fc4c0937bd">

RESULT
squareMeters         0
numberOfRooms        0
hasYard              0
hasPool              0
floors               0
cityCode             0
cityPartRange        0
numPrevOwners        0
made                 0
isNewBuilt           0
hasStormProtector    0
basement             0
attic                0
garage               0
hasStorageRoom       0
hasGuestRoom         0
price                0
category             0
dtype: int64

# Duplicated rows and outliers
The next step in the data-cleaning process is to check for duplicate rows. This step is important because having duplicate rows can lead to biased analysis or model training.

<img width="235" alt="image" src="https://github.com/Jhonnatan7br/Hard-Luxury-Paris-Housing/assets/104907786/e0fa5bf9-3148-4688-bc25-768baa61fb74">

RESULT
0

In this case, the dataset does not contain any duplicate rows. This is another positive aspect, as we don't need to remove any duplicates.

Another common data-cleaning step is to look for outliers, which are values that are significantly different from the other values in the dataset. Outliers can sometimes indicate errors or unusual circumstances, and they can affect the results of some types of analysis or model training. However, deciding what to do about outliers can be complex and depends on the specifics of your dataset and your goals.

Let's use a boxplot to visually check for outliers in the price column, which is likely to be one of the most important columns in this dataset.

<img width="369" alt="image" src="https://github.com/Jhonnatan7br/Hard-Luxury-Paris-Housing/assets/104907786/ebe963b4-3452-48b3-abda-00b8a10d83d7">

The boxplot for the 'price' column does not show any obvious outliers.

The boxplot displays a summary of the distribution of values in the 'price' column. The box itself represents the interquartile range (IQR), which is the range between the first quartile (25th percentile) and the third quartile (75th percentile). The line inside the box is the median (50th percentile). The "whiskers" extending from the box represent the range of values within 1.5 times the IQR. Points beyond the whiskers are often considered potential outliers, but there don't appear to be any such points for the 'price' column in this dataset.

# Data types of each column
Another important step in data cleaning is checking the data types of each column to make sure they're appropriate for the data they contain. For example, categorical data should usually be of 'object' or 'category' data type, while numerical data should be 'int64' or 'float64'. Let's check the data types in this dataset.

RESULT
squareMeters           int64
numberOfRooms          int64
hasYard                int64
hasPool                int64
floors                 int64
cityCode               int64
cityPartRange          int64
numPrevOwners          int64
made                   int64
isNewBuilt             int64
hasStormProtector      int64
basement               int64
attic                  int64
garage                 int64
hasStorageRoom         int64
hasGuestRoom           int64
price                float64
category              object
dtype: object

The data types for each column are as follows:

Most of the columns are of type 'int64', which is appropriate for numerical data, particularly since these columns appear to contain integer values.
The 'price' column is of type 'float64', which is appropriate for numerical data that can take on any real number value.
The 'category' column is of type 'object', which is the general type for strings in pandas. This is appropriate for a column that contains categorical data as strings.
Given these data types, it doesn't look like there's any need for conversion.

# Inconsistent or incorrect values

Another data cleaning step that's often overlooked but can be very important is checking for inconsistent or incorrect values. For example, for the 'made' column, we would not expect to see any years in the future (after the current year). Let's check for this kind of error.

<img width="448" alt="image" src="https://github.com/Jhonnatan7br/Hard-Luxury-Paris-Housing/assets/104907786/f2d24926-0180-43fb-b9b8-e253f938bd84">

Empty DataFrame
Columns: [squareMeters, numberOfRooms, hasYard, hasPool, floors, cityCode, cityPartRange, numPrevOwners, made, isNewBuilt, hasStormProtector, basement, attic, garage, hasStorageRoom, hasGuestRoom, price, category]
Index: []

The 'made' column does not contain any years after the current year (2023), so there are no inconsistencies in this regard.

This dataset appears to be quite clean already, which is not always the case in real-world data science projects. However, it's always a good idea to perform these checks to ensure the quality of the dataset before moving on to the analysis or model training stage.

The steps we've gone through here represent a basic data cleaning process. Depending on the dataset and the specific goals of your project, there might be additional steps you would want to take, such as:

Encoding categorical variables: If you plan to use machine learning algorithms, you might need to convert categorical variables into a format that the algorithm can understand.
Feature scaling: Some machine learning algorithms require the input features to be on a similar scale.
Imputing missing values: If the dataset had missing values, we would need to decide how to handle them. Options include deleting rows or columns with missing values, or imputing the missing values based on the other values in the column.
Feature engineering: You might want to create new features based on the existing ones to better capture the relationships in the data.

# Data transformation

Data transformation is a crucial step in many data analysis and machine learning workflows. Here are some common data transformations we can perform on the provided dataset:

- Normalization: Scale numeric features so they have a mean of 0 and a standard deviation of 1.
- One-Hot Encoding: Convert the categorical feature 'category' (Basic or Luxury) into separate binary columns.
- Feature Engineering: Create new features, like a "total area" feature by summing up squareMeters, basement, attic, and garage.
- Log Transformation: Apply a log transformation to the 'price' column to handle its skewed distribution.
- Binning: Convert continuous features like price into bins or categories.


RESULT
   squareMeters  numberOfRooms  hasYard  hasPool  floors  cityCode  \
0         75523              3        0        1      63      9373   
1         80771             39        1        1      98     39381   
2         55712             58        0        1      19     34457   
3         32316             47        0        0       6     27939   
4         70429             19        1        1      90     38045   

   cityPartRange  numPrevOwners  made  isNewBuilt  ...  hasStorageRoom  \
0              3              8  2005           0  ...               0   
1              8              6  2015           1  ...               1   
2              6              8  2021           0  ...               1   
3             10              4  2012           0  ...               0   
4              3              7  1990           1  ...               1   

   hasGuestRoom      price  category  squareMeters_normalized  category_Basic  \
0             7  7559081.5     Basic                 0.891518               1   
1             2  8085989.5    Luxury                 1.073902               0   
2             9  5574642.1     Basic                 0.203023               1   
3             3  3232561.2    Luxury                -0.610061               0   
4             4  7055052.0    Luxury                 0.714485               0   

   category_Luxury total_area  price_log  price_bin  
0                0      89797  15.838260       High  
1                1      86988  15.905644       High  
2                0      67636  15.533739     Medium  
3                1      40475  14.988786        Low  
4                1      81585  15.769255       High  

[5 rows x 24 columns]

Here's a summary of the applied data transformationS:

- Normalization: We normalized the squareMeters column, and the result is stored in the squareMeters_normalized column. This ensures that the values have a mean of 0 and a standard deviation of 1.
- One-Hot Encoding: We converted the category column into two separate binary columns: category_Basic and category_Luxury. These columns have a value of 1 if the house belongs to the respective category and 0 otherwise.
- Feature Engineering: We created a new total_area column by summing up the squareMeters, basement, attic, and garage columns. This provides a comprehensive metric for the total usable area of a property.
- Log Transformation: We applied a log transformation to the price column to handle any skewed distribution, and the result is stored in the price_log column.
- Binning: We converted the price column into three categories (Low, Medium, High) based on its quantiles, and the result is stored in the price_bin column.

These transformations have augmented the dataset, providing new perspectives and potential features for further analysis or modeling.
